{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":93986282,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CSCA-5642: Monet GAN Image Generation Project #\n#### Develop a GAN neural network with a generator model and a discriminator model to generate 7000 to 10,000 images from an input set of 7000 photos. ####\n    \n* Author: Alexander Meau  \n* Email: alme9155@colorado.edu  \n* GitHub: [https://github.com/alme9155/csca-5642-week5/tree/main](https://github.com/alme9155/csca-5642-week5/tree/main)  ","metadata":{"editable":false}},{"cell_type":"markdown","source":"## I. Brief Description of the Project and Data ##\n\nThis project aims to create a generative adversarial network (GAN) that generates images resembling Monet's paintings.\n- Submission will be evaluated by inception score MiFID(Memorization-informed Fréchet Inception Distance)\n- MiFID score is a modification of Fréchet Inception Distance (FID).\n- The smaller the MiFID value, the better the generated images will be. \n\n### Dataset: ####\n* The dataset contains 4 data directories: \"monet_tfrec\", \"photo_tfrec\", \"monet_jpg\", \"photo_jpg\"\n* The monet_tfrec and monet_jpg directories contain the same painting images\n* The photo_tfrec and photo_jpg directories contain the same photos.\n\n|Directory        | Dimension      | Size      | Structure   |\n|:----------------|----------------|:----------|:-----------:|\n| monet_tfrec | 300   | 256x256 | JPEG |\n| photo_tfrec | 300   | 256x256 | JPEG |\n| monet_jpg   | 7028  | 256x256 | JPEG |\n| photo_jpg   | 7028  | 256x256 | JPEG |\n\n\n### Competition Rules ###\n* Create an Images.zip that contains 7,000-10,000 images sized 256x256.","metadata":{"editable":false}},{"cell_type":"markdown","source":"## II. Exploratory Data Analysis (EDA) ##\n- Review the total number of monet paintings in tref format (File Count)\n- Review the total number of photos in tfref format (File Count)\n- Inspect image dimension (width x height) of monets paintings\n- Inspect image dimension (width x height) of photos\n- Display sample monet paints and photos","metadata":{"editable":false}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under t 4ehe input directory\n\nimport os\n\njpg_file_count =0\ntfrec_file_count = 0\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    print(f\"[{dirname}]\")\n\n    for filename in filenames:\n        if filename.endswith('.jpg'):\n            jpg_file_count +=1\n            if jpg_file_count < 3:\n                print(\"\\t\"+os.path.join(dirname, filename))\n            elif jpg_file_count == 3:\n                print(\"\\t...\")\n        elif filename.endswith('.tfrec'):\n            tfrec_file_count +=1\n            if tfrec_file_count < 3:\n                print(\"\\t\"+os.path.join(dirname, filename))\n            elif tfrec_file_count == 3:\n                print(\"\\t...\")\n        else:        \n            print(os.path.join(dirname, filename))\n    if jpg_file_count > 0:\n        print(f\"\\n\\tTotal number of .jpg files in {dirname}: {jpg_file_count}\")\n        jpg_file_count =0\n    if tfrec_file_count > 0:\n        print(f\"\\n\\tTotal number of .tfrec files in {dirname}: {tfrec_file_count}\")\n        tfrec_file_count =0\n        \n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display sample files\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport cv2\nfrom kaggle_datasets import KaggleDatasets\n\nINPUT_DIR = \"/kaggle/input/gan-getting-started\"\nmonet_jpg_dir = os.path.join(INPUT_DIR, 'monet_jpg')\nmonet_tfrec_dir = os.path.join(INPUT_DIR, 'monet_tfrec')\nphoto_jpg_dir = os.path.join(INPUT_DIR, 'photo_jpg')\nphoto_tfrec_dir = os.path.join(INPUT_DIR, 'photo_tfrec')\n\n# Show five random photo images\ndef show_random_photos(folder, label, n=5):\n    sample_files = random.sample(os.listdir(folder), n)\n    plt.figure(figsize=(15, 3))\n    for i, file in enumerate(sample_files):\n        img = cv2.imread(os.path.join(folder, file))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.subplot(1, n, i+1)\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(f\"{label} {i+1}\")\n    plt.tight_layout()\n    plt.show()\n\n\n# Run both\nprint(\"Showing 5 random Photo images:\")\nshow_random_photos(photo_jpg_dir, 'Photo', n=5)\nprint(\"Showing 5 random Monet paintings:\")\nshow_random_photos(monet_jpg_dir, 'Monet Painting', n=5)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# examine images in data directories\nimport os\nimport cv2\nfrom PIL import Image\n\n# data directory\nINPUT_DIR = \"/kaggle/input/gan-getting-started\"\nmonet_jpg_dir = os.path.join(INPUT_DIR, 'monet_jpg')\nmonet_tfrec_dir = os.path.join(INPUT_DIR, 'monet_tfrec')\nphoto_jpg_dir = os.path.join(INPUT_DIR, 'photo_jpg')\nphoto_tfrec_dir = os.path.join(INPUT_DIR, 'photo_tfrec')\n\nEXPECTED_SIZE = (256, 256)\nEXPECTED_MODE = \"RGB\"  \nPIXEL_RANGE = (-1.0, 1.0)\n\ndef examine_files(image_dir):\n    for filename in os.listdir(image_dir):\n        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n            img_path = os.path.join(image_dir, filename)\n            with Image.open(img_path) as img:\n                # Check size and mode\n                if img.size != EXPECTED_SIZE or img.mode != EXPECTED_MODE:\n                    print(\"Needs resizing and preprocessing:\")\n                    print(f\"File: {filename}\")\n                    print(f\"Size: {img.size}, Mode: {img.mode}\")\n                    break\n                    \n                image_np = np.asarray(img).astype(np.float32)\n                image_np = (image_np / 127.5) - 1.0 \n                \n                # Check pixel range\n                if not (np.min(image_np) >= -1.0 and np.max(image_np) <= 1.0):\n                    print(\"Pixel values in {filename} out of range [-1, 1]:\")\n                    print(f\"File: {filename}\")\n                    print(f\"Pixel min: {np.min(image_np)}, max: {np.max(image_np)}\")\n                    break\n    else:\n        print(f\"All images in {image_dir} are (Dimension) 256×256, (Channel) RGB, and pixel range to [-1, 1]\")\n        print(f\"No further data pre-processing is required for {image_dir}\")\n\nexamine_files(monet_jpg_dir)\nexamine_files(photo_jpg_dir)\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Brief Conclusion of EDA ###\n\n* All image files in the corresponding image directory are already normalized to the required output dimensions 256x256x3 channels.\n* No further pre-processing is required before applying an adversarial generative neural network.\n  - [ monet_tfrec ] : 300 Monet paintings sized 256x256 with RGB channels in JPEG format\n  - [ photo_tfrec ]: 300 Monet paintings sized 256x256 with RGB channels in TFRecord format\n  - [ monet_jpg ] : 7028 photos sized 256x256 with RGB channels in JPEG format\n  - [ photo_jpg ] : 7028 photos sized 256x256 with RGB channels in TFRecord format","metadata":{"editable":false}},{"cell_type":"markdown","source":"## III. Generative Neural Network Architecture: CycleGAN ##\n\n### Model Description ###\n- Following the tutorial on the Kaggle completion, I have decided to use the CycleGAN architecture for this project.\n- CycleGAN is a dual DCGAN design (Deep Convolution GAN) to deploy two generators and two discriminators.\n- This bi-directional setup will enable translation between domains: A-> B and B -> A.\n- - Key Components of CycleGAN are to deploy two generators and two discriminators in the setup.\n  - Two Generators:\n      - G: X -> Y (e.g., Photo → Monet)\n      - F: Y -> X (e.g., Monet → Photo)\n  - Two Discriminators:\n      - D_Y distinguish real Y from fake Y (G(X)) (e.g. real Monet)\n      - D_X distinguish real X from fake X (F(Y)) (e.g. real Photo)\n  - By applying a cycle-consistency constraint, it ensures the generated image not only carries Monet's impressionistic style but also looks realistic.\n  - Otherwise, the generated image of a zebra might have an extra limb\n\nRef: [https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook](https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook)","metadata":{"editable":false}},{"cell_type":"code","source":"# CycleGAN class definition\nimport tensorflow as tf\nfrom tensorflow.keras import layers, optimizers\nimport time\n\nclass CycleGAN(tf.keras.Model):\n    def __init__(self, img_size=256, lambda_cycle=10):\n        super(CycleGAN, self).__init__()\n        self.img_size = img_size\n        self.lambda_cycle = lambda_cycle\n        self.loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)     \n        self.build_models()\n        self.gen_learning_rate=2e-4\n        self.disc_learning_rate=2e-4\n        print('CycleGAN::Init() complete.')\n\n    def residual_block(self, filters):\n        return [\n            layers.Conv2D(filters, kernel_size=3, padding='same', use_bias=False),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Conv2D(filters, kernel_size=3, padding='same', use_bias=False),\n            layers.BatchNormalization()\n        ]\n    \n    def build_generator(self):\n        # kernel size: 256 x 256 x RGB channel\n        inputs = tf.keras.layers.Input(shape=[self.img_size, self.img_size, 3])\n        x = inputs\n\n        # down stack\n        for layer in [\n            layers.Conv2D(64, 4, strides=2, padding='same', use_bias=False),\n            layers.LeakyReLU(),\n            layers.Conv2D(128, 4, strides=2, padding='same', use_bias=False),\n            layers.BatchNormalization(),\n            layers.LeakyReLU(),\n            layers.Conv2D(256, 4, strides=2, padding='same', use_bias=False),\n            layers.BatchNormalization(),\n            layers.LeakyReLU(),\n        ]:\n            x = layer(x)\n\n        # residual block\n        for _ in range(6):\n            res = x\n            for layer in self.residual_block(256):\n                res = layer(res)\n            x = layers.Add()([x, res])\n\n        # up stack\n        for layer in [\n            layers.Conv2DTranspose(128, 4, strides=2, padding='same', use_bias=False),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n            layers.Conv2DTranspose(64, 4, strides=2, padding='same', use_bias=False),\n            layers.BatchNormalization(),\n            layers.ReLU(),\n        ]:\n            x = layer(x)\n\n        #last output\n        x = tf.keras.layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='tanh')(x)\n        return tf.keras.Model(inputs=inputs, outputs=x)\n\n    def build_discriminator(self):\n        # discriminator only has down stack with final output\n        inputs = layers.Input(shape=[self.img_size, self.img_size, 3])\n        x = inputs\n\n        # down stack\n        for layer in [\n            layers.Conv2D(64, 4, strides=2, padding='same', use_bias=False),\n            layers.LeakyReLU(),\n            layers.Conv2D(128, 4, strides=2, padding='same', use_bias=False),\n            layers.BatchNormalization(),\n            layers.LeakyReLU(),\n            layers.Conv2D(256, 4, strides=2, padding='same', use_bias=False),\n            layers.BatchNormalization(),\n            layers.LeakyReLU(),\n            layers.Conv2D(512, 4, strides=2, padding='same', use_bias=False),\n            layers.BatchNormalization(),\n            layers.LeakyReLU(),\n            layers.Conv2D(1, 4, strides=1, padding='same') # Outputs BinaryCrossentropy(from_logits=True).\n        ]:\n            x = layer(x)            \n        return tf.keras.Model(inputs=inputs, outputs=x)\n\n    def build_models(self):\n        self.gen_photo_to_monet = self.build_generator()\n        self.gen_monet_to_photo = self.build_generator()\n        self.disc_photo = self.build_discriminator()\n        self.disc_monet = self.build_discriminator()\n\n    def generator_loss(self, discriminator_output):\n        return self.loss_obj(tf.ones_like(discriminator_output), discriminator_output)\n\n    def discriminator_loss(self, real_output, fake_output):\n        real_loss = self.loss_obj(tf.ones_like(real_output), real_output)\n        fake_loss = self.loss_obj(tf.zeros_like(fake_output), fake_output)\n        return (real_loss + fake_loss) * 0.5\n\n    \n    def compile(self, gen_learning_rate=None, disc_learning_rate=None):\n        super().compile()\n        gen_lr = gen_learning_rate if gen_learning_rate is not None else self.gen_learning_rate\n        disc_lr = disc_learning_rate if disc_learning_rate is not None else self.disc_learning_rate\n        self.gen_photo_to_monet_optimizer = optimizers.Adam(gen_lr, beta_1=0.5)\n        self.gen_monet_to_photo_optimizer = optimizers.Adam(gen_lr, beta_1=0.5)\n        self.disc_photo_optimizer = optimizers.Adam(disc_lr, beta_1=0.5)\n        self.disc_monet_optimizer = optimizers.Adam(disc_lr, beta_1=0.5)          \n        print('CycleGAN::compile() complete.')\n\n    @tf.function\n    def train_step(self, batch_data):\n        real_photo, real_monet = batch_data\n\n        with tf.GradientTape(persistent=True) as tape:\n            # Generate images\n            fake_monet = self.gen_photo_to_monet(real_photo, training=True)\n            fake_photo = self.gen_monet_to_photo(real_monet, training=True)\n            cycled_photo = self.gen_monet_to_photo(fake_monet, training=True)\n            cycled_monet = self.gen_photo_to_monet(fake_photo, training=True)\n            same_photo = self.gen_monet_to_photo(real_photo, training=True)\n            same_monet = self.gen_photo_to_monet(real_monet, training=True)\n\n            # Discriminator outputs\n            disc_real_photo = self.disc_photo(real_photo, training=True)\n            disc_fake_photo = self.disc_photo(fake_photo, training=True)\n            disc_real_monet = self.disc_monet(real_monet, training=True)\n            disc_fake_monet = self.disc_monet(fake_monet, training=True)\n\n            # Generator losses\n            gen_photo_to_monet_loss = self.generator_loss(disc_fake_monet)\n            gen_monet_to_photo_loss = self.generator_loss(disc_fake_photo)\n            cycle_loss = tf.reduce_mean(tf.abs(real_photo - cycled_photo)) + tf.reduce_mean(tf.abs(real_monet - cycled_monet))\n            identity_loss = tf.reduce_mean(tf.abs(real_photo - same_photo)) + tf.reduce_mean(tf.abs(real_monet - same_monet))\n\n            total_gen_photo_to_monet_loss = gen_photo_to_monet_loss + self.lambda_cycle * cycle_loss + 0.5 * self.lambda_cycle * identity_loss\n            total_gen_monet_to_photo_loss = gen_monet_to_photo_loss + self.lambda_cycle * cycle_loss + 0.5 * self.lambda_cycle * identity_loss\n\n            # Discriminator losses\n            disc_photo_loss = self.discriminator_loss(disc_real_photo, disc_fake_photo)\n            disc_monet_loss = self.discriminator_loss(disc_real_monet, disc_fake_monet)\n\n        # Gradients\n        grads_gen_photo_to_monet = tape.gradient(total_gen_photo_to_monet_loss, self.gen_photo_to_monet.trainable_variables)\n        grads_gen_monet_to_photo = tape.gradient(total_gen_monet_to_photo_loss, self.gen_monet_to_photo.trainable_variables)\n        grads_disc_photo = tape.gradient(disc_photo_loss, self.disc_photo.trainable_variables)\n        grads_disc_monet = tape.gradient(disc_monet_loss, self.disc_monet.trainable_variables)\n\n        # Clip gradients\n        grads_gen_photo_to_monet = [tf.clip_by_norm(g, 1.0) for g in grads_gen_photo_to_monet]\n        grads_gen_monet_to_photo = [tf.clip_by_norm(g, 1.0) for g in grads_gen_monet_to_photo]\n        grads_disc_photo = [tf.clip_by_norm(g, 1.0) for g in grads_disc_photo]\n        grads_disc_monet = [tf.clip_by_norm(g, 1.0) for g in grads_disc_monet]\n\n        # Apply gradients\n        self.gen_photo_to_monet_optimizer.apply_gradients(zip(grads_gen_photo_to_monet, self.gen_photo_to_monet.trainable_variables))\n        self.gen_monet_to_photo_optimizer.apply_gradients(zip(grads_gen_monet_to_photo, self.gen_monet_to_photo.trainable_variables))\n        self.disc_photo_optimizer.apply_gradients(zip(grads_disc_photo, self.disc_photo.trainable_variables))\n        self.disc_monet_optimizer.apply_gradients(zip(grads_disc_monet, self.disc_monet.trainable_variables))\n\n        del tape\n        return {\n            'gen_photo_to_monet_loss': total_gen_photo_to_monet_loss,\n            'gen_monet_to_photo_loss': total_gen_monet_to_photo_loss,\n            'disc_photo_loss': disc_photo_loss,\n            'disc_monet_loss': disc_monet_loss\n        }\n\n    def fit(self, photo_dataset, monet_dataset, epochs=20):\n        for epoch in range(epochs):\n            start = time.time()\n            for image_photo, image_monet in tf.data.Dataset.zip((photo_dataset, monet_dataset)):\n                self.train_step((image_photo, image_monet))\n            print(f\"Epoch {epoch+1} completed in {time.time() - start:.2f}s\")\n        print(\"CycleGAN::Training() complete.\")\n\nprint('CycleGAN Model Defined.')","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## IV. Training CycleGAN neural network ##\n\n* Load Monet and Photo Dataset\n* Train CycleGAN model with loaded dataset\n* Generate 10 Sample monet painting to be displayed side-by-side with the original photo.\n \nRef: [https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook](https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook)","metadata":{"editable":false}},{"cell_type":"code","source":"# load dataset\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport os\nimport matplotlib.pyplot as plt\n\nIMAGE_SIZE = [256, 256]\nBATCH_SIZE = 1\n\n# Get GCS path for TFRecord data\nGCS_PATH = KaggleDatasets().get_gcs_path('gan-getting-started')\n\nINPUT_DIR = \"/kaggle/input/gan-getting-started\"\nmonet_tfrec_dir = os.path.join(INPUT_DIR, 'monet_tfrec')\nphoto_tfrec_dir = os.path.join(INPUT_DIR, 'photo_tfrec')\n\n# Use it in TFRecordDataset for TPU\nmonet_tfrec_path = tf.io.gfile.glob(monet_tfrec_dir + '/*.tfrec')\nmonet_dataset = tf.data.TFRecordDataset(monet_tfrec_paths)\nphoto_tfrec_path = tf.io.gfile.glob(photo_tfrec_dir + '/*.tfrec')\nphoto_dataset = tf.data.TFRecordDataset(photo_tfrec_paths)\n\ndef decode_image(image):\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = (tf.cast(image, tf.float32) / 127.5) - 1\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\ndef load_dataset(tfrecord_paths):\n    return (tf.data.TFRecordDataset(tfrecord_paths)\n            .map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n            .cache()\n            .shuffle(1024)\n            .batch(BATCH_SIZE)\n            .prefetch(tf.data.AUTOTUNE))\n\n\nmonet_dataset = load_dataset(monet_tfrec_path)\nphoto_dataset = load_dataset(photo_tfrec_path)\n\nexample_monet = next(iter(monet_dataset))\nexample_photo = next(iter(photo_dataset))\n\nplt.subplot(121)\nplt.title('Sample Photo from dataset')\nplt.imshow(example_photo[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Sample Monet Paiting from dataset')\nplt.imshow(example_monet[0] * 0.5 + 0.5)\nprint('Monet and Photo Dataset ready.')","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport zipfile\nimport shutil\nimport numpy as np\nimport PIL.Image\nimport random\nimport matplotlib.pyplot as plt\n\nIMG_SIZE = 256\nEPOCHS = 25\nOUTPUT_DIR = \"../generated_images\"\n\nmodel = CycleGAN(img_size=IMG_SIZE)\nmodel.compile()\nhistory = model.fit(photo_dataset, monet_dataset, epochs=EPOCHS)\n\n# Remove output directory if it exists.\nif os.path.exists(OUTPUT_DIR):\n    shutil.rmtree(OUTPUT_DIR)\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Randomly pick 10 generated images, and display generated images side by side with original photos\ni = 1\nfor photo in photo_dataset.unbatch().take(2):\n    input_img = tf.expand_dims(photo, axis=0)  \n    generated = model.gen_photo_to_monet(input_img, training=False)[0]\n    \n    original_img = (photo.numpy() * 127.5 + 127.5).astype(np.uint8)\n    generated_img = (generated.numpy() * 127.5 + 127.5).astype(np.uint8)\n    PIL.Image.fromarray(generated_img).save(os.path.join(OUTPUT_DIR, f\"{i}.jpg\"))\n    \n    # Display images side by side\n    plt.figure(figsize=(6, 3))\n    plt.subplot(1, 2, 1)\n    plt.imshow(original_img)\n    plt.title(\"Original Photo\")\n    plt.axis(\"off\")\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(generated_img)\n    plt.title(\"Generated Monet\")\n    plt.axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n    i += 1\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## V. Prepare submission zip file ##\n* Generate 7000 images in jpg file\n* Zip jpg files to be submitted to the competition.","metadata":{"editable":false}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport io\nimport zipfile\nimport shutil\n\nOUTPUT_ZIPFILE = \"images.zip\"\nTMP_DIR = \"../tmp\"\nNUM_IMAGES = 7000 \n\n#Clean up temporary directory if used\nif os.path.exists(TMP_DIR):\n    shutil.rmtree(TMP_DIR)\n\n#Generate zip file\nwith zipfile.ZipFile(OUTPUT_ZIPFILE, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n    i = 1\n    print(f\"Generating [{NUM_IMAGES}] images...\")\n    for photo_batch in photo_dataset.repeat().take(NUM_IMAGES):  # Repeat dataset to generate enough images\n        generated = model.gen_photo_to_monet(photo_batch, training=False)\n        for img in generated:\n            if i > NUM_IMAGES:\n                break\n            img = (img.numpy() * 127.5 + 127.5).astype(np.uint8)\n            im = PIL.Image.fromarray(img)\n            # Save image to bytes buffer\n            buffer = io.BytesIO()\n            im.save(buffer, format=\"JPEG\")\n            # Write to zip file\n            z.writestr(f\"image_{i}.jpg\", buffer.getvalue())\n            i += 1\n\nprint(f\"Submission zip file [{OUTPUT_ZIPFILE}] ready.\")\n","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Clean up temporary directory if used\nimport os\nimport shutil\n\nif os.path.exists(TMP_DIR):\n    shutil.rmtree(TMP_DIR)\n\n\nif os.path.exists(OUTPUT_DIR):\n    shutil.rmtree(OUTPUT_DIR)\n\nif os.path.exists(OUTPUT_ZIPFILE):\n    os.remove(OUTPUT_ZIPFILE)\nprint('tmp directory removed.')","metadata":{"trusted":true,"editable":false},"outputs":[],"execution_count":null}]}